{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: PySpark - I\n",
    "### CS186, UC Berkeley, Spring 2016\n",
    "### Due: Thursday Feb 25, 2016, 11:59 PM\n",
    "### Note: This homework is to be done individually!  Do not modify any existing method signatures.\n",
    "### **This is the first of two .ipynb files in this homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## On some computers it may be possible to run this lab \n",
    "## locally by using this script; you will need to run\n",
    "## this each time you start the notebook.\n",
    "## You do not need to run this on inst machines.\n",
    "\n",
    "# from local_install import setup_environment\n",
    "# setup_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pyspark\n",
    "from utils import SparkContext as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import CleanRDD\n",
    "from utils import tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Word Count in PySpark\n",
    "\n",
    "\n",
    "Using RDD transformations, find the top 20 words in the given text file, sorted in descending order by the number of times they occur. Part of the code has been implemented for you - fill out the rest!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * BEGIN STUDENT CODE *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def countWords(input_text):\n",
    "    \"\"\"\n",
    "    Returns a python list of the top 20 words in input_text, \n",
    "    sorted in descending order by the number of times they occur.\n",
    "    \n",
    "    :param input_text: Path to text file\n",
    "    \n",
    "    >>> countWords(\"asyoulikeit.txt\")[0:3]\n",
    "    [('the', 692), ('and', 671), ('i', 638)]\n",
    "    \n",
    "    \"\"\"\n",
    "    rdd = sc.textFile(input_text) # Create an rdd containing lines from the file\n",
    "    \n",
    "    result_rdd = rdd # rdd.DO_SOME_THING_AMAZING\n",
    "    \n",
    "    python_list = result_rdd.take(20) # Note that all computation will happen here\n",
    "    return python_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * END STUDENT CODE *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a quick debugging test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "countWords(\"asyoulikeit.txt\")[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be:\n",
    "```\n",
    "[('the', 692), ('and', 671), ('i', 638)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Pyspark Fundamentals\n",
    "\n",
    "Let's now explore how an rdd is constructed. Specifically, let's implement our own textFile method, which will support custom line delimiters. We've already implemented some of the logic in `textFile` for you, but you will need to complete `readlineInPartitions`, which should yield lines that the given partition should iterate over.  Recall that RDDs have multiple partitions - you will need to partition the file appropriately.\n",
    "\n",
    "We've also written the following function for you to make life just a little easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function takes an input file handle and returns a\n",
    "# \"line\" of characters, up to but not including the \n",
    "# delimiter. We've implemented this for you :-).\n",
    "def readToDelimiter(fHandle, delimiter):\n",
    "    s = \"\"\n",
    "    while True:\n",
    "        c = fHandle.read(1)\n",
    "        if c == \"\" or c == delimiter:\n",
    "            return s\n",
    "        s += c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * BEGIN STUDENT CODE *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readlinesInPartition(totalPartitions, partitionId, filename, delimiter):\n",
    "    \"\"\"\n",
    "    Return an *iterator* over the \"valid\" lines in this partition of the file.\n",
    "    \n",
    "    :param totalPartitions: Total number of partitions in the RDD\n",
    "    :param partitionId: The index of this current partition - (0 indexed)\n",
    "    :param filename: The path to the file.\n",
    "    :param delimiter: Character used to signal stop of element chunk.\n",
    "    \"\"\"\n",
    "    \n",
    "    filesize = os.path.getsize(filename)\n",
    "    partitionSize = int(math.ceil(filesize / float(totalPartitions)))\n",
    "    \n",
    "    beginPartition = None # TODO: you do the math (it's pretty easy)\n",
    "    endPartition = None # TODO: you do the math (+1 that)\n",
    "    \n",
    "    with open(filename, \"r\") as fHandle:\n",
    "        # TODO: yield great things!\n",
    "        # Hint 1: The very first line is special\n",
    "        # Hint 2: We might find ourselves overlapping a bit\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * END STUDENT CODE *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def textFile(filename, delimiter=\"\\n\", numPartitions=2):\n",
    "    \"\"\"\n",
    "    This function should take a file and return an RDD of the lines \n",
    "    in that file.\n",
    "    \n",
    "    :param filename: The path to the file.\n",
    "    :param delimiter: Character used to signal stop of element chunk.\n",
    "    \"\"\"\n",
    "    # create a collection of partitionIds from 0 up to numPartitions\n",
    "    partitionIds = sc.parallelize(range(numPartitions), numPartitions)\n",
    "    \n",
    "    readIteratorWithIndex = lambda idx, iterator: readlinesInPartition(numPartitions, idx, filename, delimiter)\n",
    "    \n",
    "    ## Execute your readlines code on each partition\n",
    "    return partitionIds.mapPartitionsWithIndex(readIteratorWithIndex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a mini test to help debug your line reader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numPartitions = 5\n",
    "partitionId = 0\n",
    "filename = \"urls.txt\"\n",
    "delimiter = \"\\n\"\n",
    "list(readlinesInPartition(numPartitions, partitionId, filename, delimiter))[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should look like:\n",
    "```\n",
    "['http://jmhdb.cs.berkeley.edu:8000/pages/page_511.html',\n",
    " 'http://jmhdb.cs.berkeley.edu:8000/pages/page_87.html']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark Transformations\n",
    "\n",
    "Implement `filter()`, `reduceByKey()`, `flatMap()`, and `join()` using (primarily)\n",
    " - `mapPartitionsWithIndex`\n",
    " - `partitionBy`\n",
    " - `collect`\n",
    " - `zipPartitions`\n",
    " \n",
    "Methods with the `[OPTIONAL]` tag are _optional_ - they are there for guidance and/or practice and can potentially make your life easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * BEGIN STUDENT CODE *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CS186RDD(CleanRDD):\n",
    "    \n",
    "    def __init__(self, rdd):\n",
    "        \"\"\"\n",
    "        You should not modify this method.\n",
    "        \"\"\"\n",
    "        CleanRDD.__init__(self, rdd)\n",
    "        \n",
    "    def filter(self, func):\n",
    "        \"\"\"\n",
    "        Return a new RDD containing only the elements that satisfy a predicate.\n",
    "        \n",
    "        :param func: Function returning output that can be evaluated as True or False.\n",
    "        \n",
    "        >>> rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "        >>> rdd.filter(lambda x: x % 2 == 0).collect()\n",
    "        [2, 4]\n",
    "        \"\"\" \n",
    "        return self # TODO\n",
    "    \n",
    "    def reduceByKey(self, func, numPartitions=None):\n",
    "        \"\"\"\n",
    "        Merge the values for each key using an associative reduce function.\n",
    "\n",
    "        This will also perform the merging locally on each mapper before\n",
    "        sending results to a reducer, similarly to a \"combiner\" in MapReduce.\n",
    "        \n",
    "        :param func: Function given elements a, b returns one output.\n",
    "        :param numPartitions: Number of partitions expected on final RDD.\n",
    "\n",
    "        >>> from operator import add\n",
    "        >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "        >>> sorted(rdd.reduceByKey(add).collect())\n",
    "        [('a', 2), ('b', 1)]\n",
    "        \"\"\"\n",
    "        return self # TODO\n",
    "    \n",
    "    def flatMap(self, func):\n",
    "        \"\"\"\n",
    "        Return a new RDD by first applying a function to all elements of this\n",
    "        RDD, and then flattening the results.\n",
    "\n",
    "        :param func: Function given an element returns an iterable.\n",
    "        \n",
    "        >>> rdd = sc.parallelize([2, 3, 4])\n",
    "        >>> sorted(rdd.flatMap(lambda x: range(1, x)).collect())\n",
    "        [1, 1, 1, 2, 2, 3]\n",
    "        >>> sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())\n",
    "        [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]\n",
    "        \"\"\"\n",
    "        return self # TODO\n",
    "    \n",
    "    def join(self, other):\n",
    "        \"\"\"\n",
    "        Return an RDD containing all pairs of elements with matching keys in\n",
    "        C{self} and C{other}.\n",
    "\n",
    "        Each pair of elements will be returned as a (k, (v1, v2)) tuple, where\n",
    "        (k, v1) is in C{self} and (k, v2) is in C{other}.\n",
    "\n",
    "        Performs a hash join across the cluster.\n",
    "        \n",
    "        :param other: Another RDD. Expects (K,V) elements.\n",
    "        \n",
    "        >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "        >>> y = sc.parallelize([(\"a\", 2), (\"a\", 3)])\n",
    "        >>> sorted(x.join(y).collect())\n",
    "        [('a', (1, 2)), ('a', (1, 3))]\n",
    "        \"\"\"\n",
    "        return self # TODO\n",
    "    \n",
    "    ## ## ## ## ## ## ## OPTIONAL BELOW ## ## ## ## ## ## ## ## ## \n",
    "    \n",
    "    def mapPartitions(self, func):\n",
    "        \"\"\"\n",
    "        [OPTIONAL]. Return a new RDD by applying a function to each partition \n",
    "        of this RDD.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def map(self, func):\n",
    "        \"\"\"\n",
    "        [OPTIONAL]. Return a new RDD by applying a function to each element \n",
    "        of this RDD.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "def _combine(function, iterator):\n",
    "    \"\"\"\n",
    "    [OPTIONAL]. Can be in reduceByKey, to combine the values of (key, value) \n",
    "    using the given function.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def _probe(iter1, iter2):\n",
    "    \"\"\"\n",
    "    [OPTIONAL]. Probes and returns the result of joins between two partitions. \n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * END STUDENT CODE *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple test for `filter`. Feel free to add to this test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numPartitions = 5\n",
    "testcollection = range(300)\n",
    "testRDD = CS186RDD(sc.parallelize(testcollection, numPartitions))\n",
    "testRDD.filter(lambda x: x % 3 == 0).collect()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output should be as follows:\n",
    "```\n",
    "[0, 3, 6, 9, 12, 15, 18, 21, 24, 27]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet another simple test for `reduceByKey` - feel free to modify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testRDD = CS186RDD(sc.parallelize([(\"a\", 1), (\"b\", 4), (\"a\", 2), (\"b\", 6)]))\n",
    "sorted(testRDD.reduceByKey(lambda value1, value2: value1 + value2).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output should be: \n",
    "```\n",
    "[('a', 3), ('b', 10)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a test for `flatMap`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numPartitions = 5\n",
    "testcollection = range(300)\n",
    "testRDD = CS186RDD(sc.parallelize(testcollection, numPartitions))\n",
    "sorted(testRDD.flatMap(lambda x: range(1, x)).collect())[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output should be \n",
    "```\n",
    "[295, 295, 295, 295, 296, 296, 296, 297, 297, 298]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another test for you to play with - `join`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testRDD = CS186RDD(sc.parallelize([(\"a\", 1), (\"b\", 4)]))\n",
    "testRDD2 = CS186RDD(sc.parallelize([(\"a\", 2), (\"a\", 3), (\"b\", \"c\"), (\"b\", \"1\")]))\n",
    "sorted(testRDD.join(testRDD2).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output should be \n",
    "```\n",
    "[('a', (1, 2)), ('a', (1, 3)), ('b', (4, '1')), ('b', (4, 'c'))]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tests.test1(countWords)\n",
    "tests.test2TextFile(textFile)\n",
    "tests.test2Filter(CS186RDD)\n",
    "tests.test2Reduce(CS186RDD)\n",
    "tests.test2FlatMap(CS186RDD)\n",
    "tests.test2Join(CS186RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
